Collaborators: 

Calvin Chan: 304144970
Simon Zou: 804347338
Ethan Schreiber: The iterator in HeapFile.java is the one from the lab1 solution by Ethan/the TA. 

Exercise 1: Life of a SimpleDB query

Step 1: simpledb.Parser.main() and simpledb.Parser.start() simpledb.Parser.main() is the entry point for the SimpleDB system. It calls simpledb.Parser.start(). The latter performs three main actions:

	- It populates the SimpleDB catalog from the catalog text file provided by the user as argument (Database.getCatalog().loadSchema(argv[0]);).
	- For each table defined in the system catalog, it computes statistics over the data in the table by calling: TableStats.computeStatistics(), which for each table does: TableStats s = new TableStats(tableid, IOCOSTPERPAGE);
	- It processes the statements submitted by the user (processNextStatement(new ByteArrayInputStream(statementBytes));)

Step 2: simpledb.Parser.processNextStatement() 
This method takes two key actions:

	- First, it gets a physical plan for the query by invoking handleQueryStatement((ZQuery)s);
	- Then it executes the query by calling query.execute();

Step 3: simpledb.Parser.handleQueryStatement() 

	- First, it creates a new Query object
	- The program gets a logical plan using parseQueryLogicalPlan(), and a physical plan is instantiated from the logical plan and if the plan is valid, the plan is printed out.

Step 4: simpledb.Parser.parseQueryLogicalPlan()

	- First, the function gets all the tables in the FROM clause of the query plan and adds it to the scan.
	- Then, if there is a WHERE clause, which uses the processExpression function to add join conditions
	- Then, if there is a GROUP BY clause, it checks to make sure there is only one (multiple conditions are unsupported) and then does the grouping
	- Then, it looks at the SELECT clause, which picks out aggregate fields and checks for query validity
	- If there is an ORDER BY clause, the results of the query are sorted
	- Finally, the logical plan is returned

Exercise 2: IntHistogram.java

IntHistogram is implemented according to the spec, using an equi-width bucket based approach that uses an array of integers with the count of how many values fall in each bucket. The formulas for estimating the selectivity of a given operation use the formulas given in the spec and avgSelectivity is not implemented. Private member functions were written to handle inequality and equality operators and the rest of the operators were defined in terms of them.

Exercise 3: TableStats.java

The TableStats constructor makes use of several HashMaps. It first iterates through the whole given table to find the maxs and mins of each field value, which are stored in two separate HashMaps that map fieldnames to their max and min. It then creates two more HashMaps, mapping fieldnames to histograms (one for Ints and one for Strings). We then iterate through the file again, adding the values to the appropriate histograms and then write the appropriate methods using histogram's estimateSelectivity and given formulas in the spec for estimateTableCardinality and scanCost. avgSelectivity is not implemented.

Exercise 4: JoinOptimizer.java

Exercise 5: IMDB database queries

Notes: I'm not sure how many hours we spent on this but it was probably around 10 to 15. I had some trouble understanding what the first exercise was asking us to do and we had some trouble implementing IntHistogram since we weren't accounting for values that did not fit in any buckets. I ran into different kinds of errors (too many files open) trying to execute the IMDB queries. Our code currently passes test and systemtest and we made no changes to the API.